# Practical Machine Learning Assignment 

### Background
"Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset)."

Source: https://www.coursera.org/learn/practical-machine-learning/supplement/PvInj/course-project-instructions-read-first

## Getting Started

The instructions and explanations below will give an understanding of the environment setup, data loading and manipulation, model development, model testing, and model validation as executed.

### Prerequisites

Setting up work directory, file paths, and downloading files.

Then, Create work folder to extract and write data to:
if(!file.exists("./assignment")){dir.create("./assignment")}

Then, set up  file download url's:
fileUrl_training <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
fileUrl_testing <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

### Installing

Loading libraries: caret, plyr, ggplot2 (for general plotting if needed), corrgram (for variable correlation evaluation), e1071 (for support vector machine model), rpart (for dtree ... classical decision tree), rpart.plot, party (for ctree ... conditional inference tree), randomForest (for random forest model)

### Data

The training and testing data for this project are available from the following web-links:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project is  sourced from: http://groupware.les.inf.puc-rio.br/har

### Data Processing, Cleanup, and Exploration

1. Read the raw data.csv files from the folder location
2. Do Initial data exploration
3. Clean Data Step 1 - all missing data to 'NA'
4. Clean Data Step 2 - Remove non-data columns
5. Clean Data Step 3 - Remove 'all-NA' data columns
6. Model Data Partitioning (Split the training data set into two data subsets using a 70/30 split of the data (one for "training" and one for "testing" purposes).)
7. Exploratory Graph on cleaned training data (Draw a Corrgram to evaluate variable correlation and strength of correlation)

## Model Design

Four models are evaluated. A Support Vector Machine (svm), a Classical Decision Inference Tree (rpart), a Conditional Inference Tree (ctree), and a Random Forest (randomForest). Response variables are predicted for each model type, and finally the "Random Forest Model" selected for fine tuning.

For model optimization a variable importance plot is considered to select relevant model parameters. The plot evaluates performance of variables on "Model Mean Accuracy" and "Mean Gini values".  A Variable Importance Table is constructed with decreasing order of importance, to table the 10 most significant variables (roll_belt, yaw_belt, pitch_belt, magnet_dumbbell_z, pitch_forearm, magnet_dumbbell_y, and roll_forearm).

## Model Testing

The redesigned Random Forest model is then tested against the test data set, and as expected, shows slightly lower accuracy. The out-of-sample error rate (i.e. [1 - accuracy]*100) is still acceptable.

## Model Validation

The final model (modelFit4_rerun) is validated against the "pml-testing.csv" data supplied (downloaded into the test_subset data frame). The model with 100 % accuracy predicts the 20 cases as supplied. The prediction results are printed below.


